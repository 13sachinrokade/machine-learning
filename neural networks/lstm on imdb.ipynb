{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got error when loading so gone for this: https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "#for this step got error did the above below steps from above link\n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=top_words)\n",
    "\n",
    "\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 189 141\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]),len(x_train[1]),len(x_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_words not worked  do manually \n",
    "#took 1000 not got because indices[3,74] = 3078 is not in [0, 1000) eroor so taking 5000\n",
    "x_train=x_train[:5000]\n",
    "y_train=y_train[:5000]\n",
    "x_test=x_test[:5000]\n",
    "y_test=y_test[:5000]\n",
    "top_words=5000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "length=220 #taking small as i got max word length is 218 above they are actually aranged in sorted order in dataset\n",
    "from keras.preprocessing import sequence\n",
    "x_train=sequence.pad_sequences(x_train,maxlen=length)\n",
    "x_test=sequence.pad_sequences(x_test,maxlen=length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 220, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words,32, input_length=length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 52s 10ms/step - loss: 0.6810 - acc: 0.5630 - val_loss: 0.6670 - val_acc: 0.6084\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.6356 - acc: 0.6844 - val_loss: 0.6010 - val_acc: 0.6860\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 48s 10ms/step - loss: 0.5189 - acc: 0.7528 - val_loss: 0.5556 - val_acc: 0.7118\n",
      "Epoch 4/5\n",
      " 960/5000 [====>.........................] - ETA: 34s - loss: 0.4448 - acc: 0.7896"
     ]
    }
   ],
   "source": [
    "results=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plt_dynamic(x, y, y_1, ax, colors=['b']):\n",
    "    ax.plot(x, y, 'b', label=\"Train Loss\")\n",
    "    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n",
    "    if len(x)==1:\n",
    "        plt.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; \n",
    "ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "x = list(range(1,6+1))\n",
    "\n",
    "vy = results.history['val_loss']\n",
    "ty = results.history['loss']\n",
    "print(x_test.shape,len(vy),len(ty))\n",
    "#plt_dynamic(x, vy, ty, ax) #got error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 43s 9ms/step - loss: 0.3345 - acc: 0.8598 - val_loss: 0.5018 - val_acc: 0.8078\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 0.3571 - acc: 0.8476 - val_loss: 0.4820 - val_acc: 0.7982\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 48s 10ms/step - loss: 0.3094 - acc: 0.8760 - val_loss: 0.4639 - val_acc: 0.7942\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 49s 10ms/step - loss: 0.3063 - acc: 0.8754 - val_loss: 0.4863 - val_acc: 0.8148\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 0.3028 - acc: 0.8750 - val_loss: 0.4958 - val_acc: 0.7732\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 50s 10ms/step - loss: 0.2763 - acc: 0.8884 - val_loss: 0.4771 - val_acc: 0.8174\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 50s 10ms/step - loss: 0.2552 - acc: 0.8992 - val_loss: 0.4848 - val_acc: 0.7920\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 0.2399 - acc: 0.9084 - val_loss: 0.4743 - val_acc: 0.8182\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 0.2294 - acc: 0.9096 - val_loss: 0.5080 - val_acc: 0.8000\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 43s 9ms/step - loss: 0.2062 - acc: 0.9232 - val_loss: 0.5114 - val_acc: 0.8158\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.2263 - acc: 0.9122 - val_loss: 0.5352 - val_acc: 0.7918\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 44s 9ms/step - loss: 0.1909 - acc: 0.9260 - val_loss: 0.5769 - val_acc: 0.7956\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.1985 - acc: 0.9236 - val_loss: 0.6047 - val_acc: 0.7888\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.1642 - acc: 0.9400 - val_loss: 0.5842 - val_acc: 0.8006\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 45s 9ms/step - loss: 0.1975 - acc: 0.9258 - val_loss: 0.6508 - val_acc: 0.7002\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.2001 - acc: 0.9206 - val_loss: 0.6296 - val_acc: 0.8014\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 43s 9ms/step - loss: 0.1756 - acc: 0.9320 - val_loss: 0.6146 - val_acc: 0.7836\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 44s 9ms/step - loss: 0.1447 - acc: 0.9460 - val_loss: 0.6853 - val_acc: 0.7942\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 45s 9ms/step - loss: 0.1472 - acc: 0.9462 - val_loss: 0.6320 - val_acc: 0.7924\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.1479 - acc: 0.9466 - val_loss: 0.6948 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "#increasing accuarcy by trainng it more times\n",
    "results=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 14s 3ms/step\n",
      "Accuracy: 80.20%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words,32, input_length=length))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(100))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model1.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, 32, input_length=length))\n",
    "model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn after embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words, 32, input_length=length))\n",
    "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(LSTM(100))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model3.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model3.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
